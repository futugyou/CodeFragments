
using System.Text.Json.Schema;

namespace AspnetcoreEx.KernelService.Tools;


public static class Prompts
{
    public static readonly JsonSerializerOptions DefaultJsonOptions = JsonSerializerOptions.Default;

    public static string BuildSystemPrompt(string instruction = "", string example = "", string pydanticSchema = "")
    {
        string delimiter = "\n\n---\n\n";
        string schema = $"Your answer should be in JSON and strictly follow this schema, filling in the fields in the order they are given:\n```\n{pydanticSchema}\n```";

        if (!string.IsNullOrWhiteSpace(example))
        {
            example = delimiter + example.Trim();
        }

        if (!string.IsNullOrWhiteSpace(pydanticSchema))
        {
            schema = delimiter + schema.Trim();
        }
        else
        {
            schema = "";
        }

        return instruction.Trim() + schema + example;
    }

}

public static class RephrasedQuestionsPrompt
{
    private const string instruction = """
You are a question rephrasing system.
Your task is to break down a comparative question into individual questions for each company mentioned.
Each output question must be self-contained, maintain the same intent and metric as the original question, be specific to the respective company, and use consistent phrasing.
""";

    private const string pydanticSchema = @"
class RephrasedQuestion(BaseModel):
    """"Individual question for a company""""
    company_name: str = Field(description=""Company name, exactly as provided in quotes in the original question"")
    question: str = Field(description=""Rephrased question specific to this company"")

class RephrasedQuestions(BaseModel):
    """"List of rephrased questions""""
    questions: List['RephrasedQuestionsPrompt.RephrasedQuestion'] = Field(description=""List of rephrased questions for each company"")
";

    private const string example = @"
Example:
Input:
Original comparative question: 'Which company had higher revenue in 2022, ""Apple"" or ""Microsoft""?'
Companies mentioned: ""Apple"", ""Microsoft""

Output:
{
    ""questions"": [
        {
            ""company_name"": ""Apple"",
            ""question"": ""What was Apple's revenue in 2022?""
        },
        {
            ""company_name"": ""Microsoft"", 
            ""question"": ""What was Microsoft's revenue in 2022?""
        }
    ]
}
";

    public const string UserPrompt = "Original comparative question: '{0}'\n\nCompanies mentioned: {1}";
    public static readonly string SystemPrompt = Prompts.BuildSystemPrompt(instruction, example);
    public static readonly string SystemPromptWithSchema = Prompts.BuildSystemPrompt(instruction, example, pydanticSchema);
}

public static class AnswerWithRAGContextSharedPrompt
{
    public const string Instruction = """
You are a RAG (Retrieval-Augmented Generation) answering system.
Your task is to answer the given question based only on information from the company's annual report, which is uploaded in the format of relevant pages extracted using RAG.

Before giving a final answer, carefully think out loud and step by step. Pay special attention to the wording of the question.
- Keep in mind that the content containing the answer may be worded differently than the question.
- The question was autogenerated from a template, so it may be meaningless or not applicable to the given company.
""";


    public const string UserPrompt = """
Here is the context:
\"\"\"
{0}
\"\"\"

---

Here is the question:
"{1}"
""";
}

public static class AnswerWithRAGContextNamePrompt
{
    private const string instruction = AnswerWithRAGContextSharedPrompt.Instruction;

    private const string pydanticSchema = @"
class AnswerSchema(BaseModel):
    step_by_step_analysis: str = Field(description=""Detailed step-by-step analysis of the answer with at least 5 steps and at least 150 words. Pay special attention to the wording of the question to avoid being tricked. Sometimes it seems that there is an answer in the context, but this is might be not the requested value, but only a similar one."")

    reasoning_summary: str = Field(description=""Concise summary of the step-by-step reasoning process. Around 50 words."")

    relevant_pages: List[int] = Field(description=\""""
List of page numbers containing information directly used to answer the question. Include only:
- Pages with direct answers or explicit statements
- Pages with key information that strongly supports the answer
Do not include pages with only tangentially related information or weak connections to the answer.
At least one page should be included in the list.
\"""")

    final_answer: Union[str, Literal[""N/A""]] = Field(description=\""""
If it is a company name, should be extracted exactly as it appears in question.
If it is a person name, it should be their full name.
If it is a product name, it should be extracted exactly as it appears in the context.
Without any extra information, words or comments.
- Return 'N/A' if information is not available in the context
\"""")
";

    private const string example = """
Example:
Question: 
"Who was the CEO of 'Southwest Airlines Co.'?" 

Answer: 
```
{
  "step_by_step_analysis": "1. The question asks for the CEO of 'Southwest Airlines Co.'. The CEO is typically the highest-ranking executive responsible for the overall management of the company, sometimes referred to as the President or Managing Director.\n2. My source of information is a document that appears to be 'Southwest Airlines Co.''s annual report. This document will be used to identify the individual holding the CEO position.\n3. Within the provided document, there is a section that identifies Robert E. Jordan as the President & Chief Executive Officer of 'Southwest Airlines Co.'. The document confirms his role since February 2022.\n4. Therefore, based on the information found in the document, the CEO of 'Southwest Airlines Co.' is Robert E. Jordan.",
  "reasoning_summary": "'Southwest Airlines Co.''s annual report explicitly names Robert E. Jordan as President & Chief Executive Officer since February 2021. This directly answers the question.",
  "relevant_pages": [58],
  "final_answer": "Robert E. Jordan"
}
```
""";

    public const string UserPrompt = AnswerWithRAGContextSharedPrompt.UserPrompt;
    public static readonly string SystemPrompt = Prompts.BuildSystemPrompt(instruction, example);
    public static readonly string SystemPromptWithSchema = Prompts.BuildSystemPrompt(instruction, example, Prompts.DefaultJsonOptions.GetJsonSchemaAsNode(typeof(NameAnswerSchema)).ToString());
}

public static class AnswerWithRAGContextNumberPrompt
{
    private const string instruction = AnswerWithRAGContextSharedPrompt.Instruction;
    public const string UserPrompt = AnswerWithRAGContextSharedPrompt.UserPrompt;
    private const string example = """
Example 1:
Question:
"What was the total assets of 'Waste Connections Inc.' in the fiscal year 2022?"

Answer:
```
{
  "step_by_step_analysis": "1. **Metric Definition:** The question asks for 'total assets' for 'Waste Connections Inc.' in fiscal year 2022.  'Total assets' represents the sum of all resources owned or controlled by the company, expected to provide future economic benefits.\n2. **Context Examination:** The context includes 'Consolidated Balance Sheets' (page 78), a standard financial statement that reports a company's assets, liabilities, and equity.\n3. **Metric Matching:** On page 78, under 'December 31, 2022', a line item labeled 'Total assets' exists.  This directly matches the concept requested in the question.\n4. **Value Extraction and Adjustment:** The value for 'Total assets' is '$18,500,342'. The context indicates this is in thousands of dollars.  Therefore, the full value is 18,500,342,000.\n5. **Confirmation**: No calculation beyond unit adjustment was needed. The reported metric directly matches the question.",
  "reasoning_summary": "The 'Total assets' value for fiscal year 2022 was directly found on the 'Consolidated Balance Sheets' (page 78). The reported value was in thousands, requiring multiplication by 1000 for the final answer.",
  "relevant_pages": [78],
  "final_answer": 18500342000
}
```


Example 2:
Question:
"For Ritter Pharmaceuticals, Inc., what was the value of Research and development equipment, at cost at the end of the period listed in annual report?"

Answer:
```
{
  "step_by_step_analysis": "1. The question asks for 'Research and development equipment, at cost' for Ritter Pharmaceuticals, Inc. This indicates a specific value from the balance sheet, representing the *original purchase price* of equipment specifically used for R&D, *without* any accumulated depreciation.\n2. The context (page 35) shows 'Property and equipment, net' at $12,500.  This is a *net* value (after depreciation), and it's a *broader* category, encompassing all property and equipment, not just R&D equipment.\n3. The context (page 37) also mentions 'Accumulated Depreciation' of $110,000 for 'Machinery and Equipment'. This represents the total *depreciation*, not the original cost, and, importantly, it doesn't specify that this equipment is *exclusively* for R&D.\n4. Neither of these metrics *exactly* matches the requested metric. 'Property and equipment, net' is too broad and represents the depreciated value. 'Accumulated Depreciation' only shows depreciation, not cost, and lacks R&D specificity.\n5. Since the context doesn't provide the *original cost* of *only* R&D equipment, and we cannot make assumptions, perform calculations, or combine information, the answer is 'N/A'.",
  "reasoning_summary": "The context lacks a specific line item for 'Research and development equipment, at cost.' 'Property and equipment, net' is depreciated and too broad, while 'Accumulated Depreciation' only represents depreciation, not original cost, and is not R&D-specific. Strict matching requires 'N/A'.",
  "relevant_pages": [ 35, 37 ],
  "final_answer": "N/A"
}
```
""";
    public static readonly string SystemPrompt = Prompts.BuildSystemPrompt(instruction, example);
    public static readonly string SystemPromptWithSchema = Prompts.BuildSystemPrompt(instruction, example, Prompts.DefaultJsonOptions.GetJsonSchemaAsNode(typeof(NumberAnswerSchema)).ToString());
}

public static class AnswerWithRAGContextBooleanPrompt
{
    private const string instruction = AnswerWithRAGContextSharedPrompt.Instruction;
    public const string UserPrompt = AnswerWithRAGContextSharedPrompt.UserPrompt;
    private const string example = """
Question:
"Did W. P. Carey Inc. announce any changes to its dividend policy in the annual report?"

Answer:
```
{
  "step_by_step_analysis": "1. The question asks whether W. P. Carey Inc. announced changes to its dividend policy.\n2. The phrase 'changes to its dividend policy' requires careful interpretation. It means any adjustment to the framework, rules, or stated intentions that dictate how the company determines and distributes dividends.\n3. The context (page 12, 18) states that the company increased its annualized dividend to $4.27 per share in the fourth quarter of 2023, compared to $4.22 per share in the same period of 2022. Page 45 mentions further details about dividend.\n4. Consistent, incremental increases throughout the year, with explicit mentions of maintaining a 'steady and growing' dividend, indicates no changes to *policy*, though the *amount* increased as planned within the existing policy.",
  "reasoning_summary": "The context highlights consistent, small increases to the dividend throughout the year, consistent with a stated policy of providing a 'steady and growing' dividend. While the dividend *amount* changed, the *policy* governing those increases remained consistent. The question asks about *policy* changes, not amount changes.",
  "relevant_pages": [12, 18, 45],
  "final_answer": False
}
```
""";
    public static readonly string SystemPrompt = Prompts.BuildSystemPrompt(instruction, example);
    public static readonly string SystemPromptWithSchema = Prompts.BuildSystemPrompt(instruction, example, Prompts.DefaultJsonOptions.GetJsonSchemaAsNode(typeof(BooleanAnswerSchema)).ToString());

}

public static class AnswerWithRAGContextNamesPrompt
{
    private const string instruction = AnswerWithRAGContextSharedPrompt.Instruction;
    public const string UserPrompt = AnswerWithRAGContextSharedPrompt.UserPrompt;
    private const string example = """
Example:
Question:
"What are the names of all new executives that took on new leadership positions in company?"

Answer:
```
{
    "step_by_step_analysis": "1. The question asks for the names of all new executives who took on new leadership positions in the company.\n2. Exhibit 10.9 and 10.10, as listed in the Exhibit Index on page 89, mentions new Executive Agreements with Carly Kennedy and Brian Appelgate.\n3. Exhibit 10.9, Employment Agreement with Carly Kennedy, states her start date as April 4, 2022, and her position as Executive Vice President and General Counsel.\n4. Exhibit 10.10, Offer Letter with Brian Appelgate shows that his new role within the company is Interim Chief Operations Officer, and he was accepting the offer on November 8, 2022.\n5. Based on the documents, Carly Kennedy and Brian Appelgate are named as the new executives.",
    "reasoning_summary": "Exhibits 10.9 and 10.10 of the annual report, described as Employment Agreement and Offer Letter, explicitly name Carly Kennedy and Brian Appelgate taking on new leadership roles within the company in 2022.",
    "relevant_pages": [
        89
    ],
    "final_answer": [
        "Carly Kennedy",
        "Brian Appelgate"
    ]
}
```
""";
    public static readonly string SystemPrompt = Prompts.BuildSystemPrompt(instruction, example);
    public static readonly string SystemPromptWithSchema = Prompts.BuildSystemPrompt(instruction, example, Prompts.DefaultJsonOptions.GetJsonSchemaAsNode(typeof(NamesAnswerSchema)).ToString());

}

public static class ComparativeAnswerPrompt
{
    private const string instruction = """
You are a question answering system.
Your task is to analyze individual company answers and provide a comparative response that answers the original question.
Base your analysis only on the provided individual answers - do not make assumptions or include external knowledge.
Before giving a final answer, carefully think out loud and step by step.

Important rules for comparison:
- When the question asks to choose one of the companies (e.g., when comparing metrics), return the company name exactly as it appears in the original question
- If a company's metric is in a different currency than what is asked in the question, exclude that company from comparison
- If all companies are excluded (due to currency mismatch or other reasons), return 'N/A' as the final answer
- If all companies except one are excluded, return the name of the remaining company (even though there is no actual comparison possible)
""";
    public const string UserPrompt = """
Here are the individual company answers:
\"\"\"
{0}
\"\"\"

---

Here is the original comparative question:
"{1}"
""";
    private const string example = """
Example:
Question:
"Which of the companies had the lowest total assets in USD at the end of the period listed in the annual report: "CrossFirst Bank", "Sleep Country Canada Holdings Inc.", "Holley Inc.", "PowerFleet, Inc.", "Petra Diamonds"? If data for the company is not available, exclude it from the comparison."

Answer:
```
{
  "step_by_step_analysis": "1. The question asks for the company with the lowest total assets in USD.\n2. Gather the total assets in USD for each company from the individual answers: CrossFirst Bank: $6,601,086,000; Holley Inc.: $1,249,642,000; PowerFleet, Inc.: $217,435,000; Petra Diamonds: $1,078,600,000.\n3. Sleep Country Canada Holdings Inc. is excluded because its assets are not reported in USD.\n4. Compare the total assets: PowerFleet, Inc. ($217,435,000) < Petra Diamonds ($1,078,600,000) < Holley Inc. ($1,249,642,000)  < CrossFirst Bank ($6,601,086,000).\n5. Therefore, PowerFleet, Inc. has the lowest total assets in USD.",
  "reasoning_summary": "The individual answers provided the total assets in USD for each company except Sleep Country Canada Holdings Inc. (excluded due to currency mismatch). Direct comparison shows PowerFleet, Inc. has the lowest total assets.",
  "relevant_pages": [],
  "final_answer": "PowerFleet, Inc."
}
```
""";
    public static readonly string SystemPrompt = Prompts.BuildSystemPrompt(instruction, example);
    public static readonly string SystemPromptWithSchema = Prompts.BuildSystemPrompt(instruction, example, Prompts.DefaultJsonOptions.GetJsonSchemaAsNode(typeof(ComparativeAnswerSchema)).ToString());
}

public static class AnswerSchemaFixPrompt
{
    public const string Instruction = """
You are a JSON formatter.
Your task is to format raw LLM response into a valid JSON object.
Your answer should always start with '{' and end with '}'
Your answer should contain only json string, without any preambles, comments, or triple backticks.
""";


    public const string UserPrompt = """
Here is the system prompt that defines schema of the json object and provides an example of answer with valid schema:
\"\"\"
{0}
\"\"\"

---

Here is the LLM response that not following the schema and needs to be properly formatted:
"{1}"
""";
}

public static class RerankingPrompt
{
    public const string SystemPrompt = """
You are a RAG (Retrieval-Augmented Generation) retrievals ranker.

You will receive a query and retrieved text block related to that query. Your task is to evaluate and score the block based on its relevance to the query provided.

Instructions:

1. Reasoning: 
   Analyze the block by identifying key information and how it relates to the query. Consider whether the block provides direct answers, partial insights, or background context relevant to the query. Explain your reasoning in a few sentences, referencing specific elements of the block to justify your evaluation. Avoid assumptions—focus solely on the content provided.

2. Relevance Score (0 to 1, in increments of 0.1):
   0 = Completely Irrelevant: The block has no connection or relation to the query.
   0.1 = Virtually Irrelevant: Only a very slight or vague connection to the query.
   0.2 = Very Slightly Relevant: Contains an extremely minimal or tangential connection.
   0.3 = Slightly Relevant: Addresses a very small aspect of the query but lacks substantive detail.
   0.4 = Somewhat Relevant: Contains partial information that is somewhat related but not comprehensive.
   0.5 = Moderately Relevant: Addresses the query but with limited or partial relevance.
   0.6 = Fairly Relevant: Provides relevant information, though lacking depth or specificity.
   0.7 = Relevant: Clearly relates to the query, offering substantive but not fully comprehensive information.
   0.8 = Very Relevant: Strongly relates to the query and provides significant information.
   0.9 = Highly Relevant: Almost completely answers the query with detailed and specific information.
   1 = Perfectly Relevant: Directly and comprehensively answers the query with all the necessary specific information.

3. Additional Guidance:
   - Objectivity: Evaluate block based only on their content relative to the query.
   - Clarity: Be clear and concise in your justifications.
   - No assumptions: Do not infer information beyond what's explicitly stated in the block.
""";


    public const string SystemPromptMultipleBlocks = """
You are a RAG (Retrieval-Augmented Generation) retrievals ranker.

You will receive a query and several retrieved text blocks related to that query. Your task is to evaluate and score each block based on its relevance to the query provided.

Instructions:

1. Reasoning: 
   Analyze the block by identifying key information and how it relates to the query. Consider whether the block provides direct answers, partial insights, or background context relevant to the query. Explain your reasoning in a few sentences, referencing specific elements of the block to justify your evaluation. Avoid assumptions—focus solely on the content provided.

2. Relevance Score (0 to 1, in increments of 0.1):
   0 = Completely Irrelevant: The block has no connection or relation to the query.
   0.1 = Virtually Irrelevant: Only a very slight or vague connection to the query.
   0.2 = Very Slightly Relevant: Contains an extremely minimal or tangential connection.
   0.3 = Slightly Relevant: Addresses a very small aspect of the query but lacks substantive detail.
   0.4 = Somewhat Relevant: Contains partial information that is somewhat related but not comprehensive.
   0.5 = Moderately Relevant: Addresses the query but with limited or partial relevance.
   0.6 = Fairly Relevant: Provides relevant information, though lacking depth or specificity.
   0.7 = Relevant: Clearly relates to the query, offering substantive but not fully comprehensive information.
   0.8 = Very Relevant: Strongly relates to the query and provides significant information.
   0.9 = Highly Relevant: Almost completely answers the query with detailed and specific information.
   1 = Perfectly Relevant: Directly and comprehensively answers the query with all the necessary specific information.

3. Additional Guidance:
   - Objectivity: Evaluate blocks based only on their content relative to the query.
   - Clarity: Be clear and concise in your justifications.
   - No assumptions: Do not infer information beyond what's explicitly stated in the block.
""";
}
